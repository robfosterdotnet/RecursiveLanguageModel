<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLM Architecture - LinkedIn Post (Medium)</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 700px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .meta {
            color: #666;
            font-size: 14px;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #0a66c2;
        }

        p {
            margin-bottom: 16px;
            color: #333;
        }

        strong {
            font-weight: 600;
            color: #000;
        }

        .list-item {
            margin-left: 20px;
            margin-bottom: 12px;
            color: #333;
        }

        .list-item::before {
            content: "→ ";
            color: #0a66c2;
            font-weight: bold;
            margin-right: 8px;
        }

        ol {
            margin-left: 30px;
            margin-bottom: 16px;
        }

        ol li {
            margin-bottom: 8px;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }

        .emphasis {
            font-weight: 600;
            font-size: 18px;
            line-height: 1.4;
            margin: 20px 0;
        }

        .hashtags {
            color: #0a66c2;
            font-weight: 500;
            margin-top: 30px;
        }

        .instructions {
            background: #f0f8ff;
            border-left: 4px solid #0a66c2;
            padding: 20px;
            margin-top: 40px;
            border-radius: 4px;
        }

        .instructions h3 {
            margin-top: 0;
            color: #0a66c2;
            font-size: 18px;
            margin-bottom: 15px;
        }

        .instructions ol {
            margin-top: 10px;
        }

        hr {
            border: none;
            height: 2px;
            background: linear-gradient(to right, #0a66c2, transparent);
            margin: 30px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="meta">
            <strong>~450 words | Best for thought leadership</strong>
        </div>

        <p class="emphasis">24 parallel LLM calls. Zero context rot. 100-page contracts analyzed with full attention.</p>

        <p>Here's the architecture.</p>

        <p>I kept hitting the same problem with legal document analysis:</p>

        <p>LLMs pay attention to the beginning and end of context.</p>

        <p>The middle? It fades.</p>

        <p>Your indemnification clause on page 47? The model is basically skimming.</p>

        <p>RAG doesn't fix this either:</p>

        <div class="list-item">Top-K retrieval is query-dependent tunnel vision</div>
        <div class="list-item">Misses cross-document relationships</div>
        <div class="list-item">Can't synthesize scattered information</div>

        <p>So I built something different.</p>

        <p><strong>Recursive Language Models (RLM):</strong></p>

        <ol>
            <li>Chunk documents at paragraph boundaries (~1800 chars)</li>
            <li>Each chunk gets its own LLM sub-call</li>
            <li>Sub-model returns: { relevant, summary, citations }</li>
            <li>Root model aggregates all findings</li>
            <li>Rewrite pass for legal precision + plain language</li>
        </ol>

        <p class="emphasis">The key insight:</p>

        <p>Each sub-call has FULL attention on its chunk.</p>

        <p>No middle-of-context loss.</p>

        <p>The root model sees distilled findings, not raw text.</p>

        <p>Irrelevant chunks get filtered (relevant: false).</p>

        <p>Citations preserved through the entire pipeline.</p>

        <hr>

        <p>I added a knowledge graph layer for complex contracts:</p>

        <p><strong>Entity extraction:</strong></p>
        <div class="list-item">parties, dates, amounts, clauses</div>
        <div class="list-item">obligations, rights, conditions</div>

        <p><strong>Relationship extraction:</strong></p>
        <div class="list-item">has_obligation, has_right</div>
        <div class="list-item">depends_on, references</div>
        <div class="list-item">effective_on, expires_on</div>

        <p>Entities merge across chunks using Jaccard similarity ≥ 0.7.</p>

        <p>"Acme Corp" in chunk 3 = "Acme Corporation" in chunk 47.</p>

        <p>The root model receives findings + graph summary.</p>

        <p><strong>Now I can answer:</strong></p>

        <p>"What obligations does Party A have that expire before the effective date of Section 12?"</p>

        <p>With citations. Across 200 chunks.</p>

        <hr>

        <p><strong>The numbers:</strong></p>

        <div class="list-item">Chunk size: 1800 chars</div>
        <div class="list-item">Max sub-calls: 24 parallel</div>
        <div class="list-item">Sub-model: lightweight (cost efficient)</div>
        <div class="list-item">Root model: full capability</div>
        <div class="list-item">Similarity threshold: 0.7</div>

        <p>Cost: ~3x a single large-context call for 24x coverage.</p>

        <hr>

        <p>Everyone's chasing bigger context windows.</p>

        <p>1M tokens. 10M. "Infinite context."</p>

        <p><strong>Wrong direction.</strong></p>

        <p>Attention doesn't scale linearly. Bigger = more diluted.</p>

        <p class="emphasis">The answer isn't bigger windows.</p>

        <p class="emphasis">It's recursive decomposition.</p>

        <p>Open sourced the implementation: robfosterdotnet/RecursiveLanguageModel</p>

        <p><strong>Four modes to compare:</strong></p>
        <div class="list-item">base (truncated)</div>
        <div class="list-item">retrieval (BM25 top-K)</div>
        <div class="list-item">rlm (sub-calls + aggregation)</div>
        <div class="list-item">rlm-graph (+ knowledge graph)</div>

        <p>Link in comments.</p>

        <p>What's your experience with context rot?</p>

        <p class="hashtags">#AI #LegalTech #LLM #MachineLearning #OpenSource</p>

        <div class="instructions">
            <h3>Posting Instructions</h3>
            <ol>
                <li>Copy text above (between the meta and instructions sections)</li>
                <li>Post WITHOUT the GitHub link in the body</li>
                <li>Immediately comment with: <code>Repo: https://github.com/robfosterdotnet/RecursiveLanguageModel</code></li>
                <li>Attach the infographic screenshot (see <code>rlm-infographic.html</code>)</li>
            </ol>
        </div>
    </div>
</body>
</html>